# Here's where you define experiment-specific hyperparameters.
# You can also create lists and group parameters together into nested sub-parts.
# In Python, this is all read as a dict.

# environment/computational parameters
seed: 32678456782       # random number generator seed (long integer value)
device: cuda            # if you have multiple GPU's, you can use 'cude:4' to specify which GPU to run on, e.g., the 4th
num_workers: 4          # number of CPU cores that load data in parallel. You can set this to the number of logical CPU cores that you have. 

# dataset parameters
data_root: /mnt/class_data/group4/katie/processed

# this is for finegrained
# train_annotations_filename: 'train_annotations_spp.json'
# val_annotations_filename: 'cis_val_annotations_spp.json'
# num_classes: 51

# this is for grouped classes
train_annotations_filename: 'train_annotations_spp_grouped.json'
val_annotations_filename: 'cis_val_annotations_spp_grouped.json'
num_classes: 28


# training hyperparameters
image_size: [224, 224]
num_epochs: 100        # number of epochs. Each epoch has multiple iterations. In each epoch the model goes over the full dataset once.
batch_size: 128         # number of images that are processed in parallel in every iteration
learning_rate: 0.001    # hyperparameter to adjust the optimizer's learning rate 
weight_decay: 0.001     # hyperparameter for regularization
patience: 5